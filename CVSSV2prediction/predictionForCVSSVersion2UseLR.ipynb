{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Software is free software released under the \"GNU General Public License v3.0\"\n",
    "\n",
    "Copyright (c) 2021 Yuning-Jiang - yuning.jiang17@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import unicodedata\n",
    "import itertools\n",
    "import logging\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from typing import KeysView\n",
    "from pandas.core.frame import DataFrame\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvdcve-1.1-2021.json.zip\n",
      "nvdcve-1.1-2020.json.zip\n",
      "nvdcve-1.1-2019.json.zip\n",
      "nvdcve-1.1-2018.json.zip\n",
      "nvdcve-1.1-2017.json.zip\n",
      "nvdcve-1.1-2016.json.zip\n",
      "nvdcve-1.1-2015.json.zip\n",
      "nvdcve-1.1-2014.json.zip\n",
      "nvdcve-1.1-2013.json.zip\n",
      "nvdcve-1.1-2012.json.zip\n",
      "nvdcve-1.1-2011.json.zip\n",
      "nvdcve-1.1-2010.json.zip\n",
      "nvdcve-1.1-2009.json.zip\n",
      "nvdcve-1.1-2008.json.zip\n",
      "nvdcve-1.1-2007.json.zip\n",
      "nvdcve-1.1-2006.json.zip\n",
      "nvdcve-1.1-2005.json.zip\n",
      "nvdcve-1.1-2004.json.zip\n",
      "nvdcve-1.1-2003.json.zip\n",
      "nvdcve-1.1-2002.json.zip\n"
     ]
    }
   ],
   "source": [
    "#Download NVD data feeds in JSON format. Ensure you have a folder called \"zip\" in the same directory.\n",
    "def get_nvd_data():\n",
    "    r = requests.get('https://nvd.nist.gov/vuln/data-feeds#JSON_FEED')\n",
    "    for filename in re.findall(\"nvdcve-1.1-[0-9]*\\.json\\.zip\",r.text):\n",
    "        print(filename)\n",
    "        r_file = requests.get(\"https://nvd.nist.gov/feeds/json/cve/1.1/\" + filename, stream=True)\n",
    "        filePath = \"zipFile\"\n",
    "        if not os.path.exists(filePath):\n",
    "            os.makedirs(filePath)\n",
    "        with open(\"zipFile/\" + filename, 'wb') as f:\n",
    "            for chunk in r_file:\n",
    "                f.write(chunk)\n",
    "get_nvd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: nvdcve-1.1-2002.json.zip\n",
      "Opening: nvdcve-1.1-2003.json.zip\n",
      "Opening: nvdcve-1.1-2004.json.zip\n",
      "Opening: nvdcve-1.1-2005.json.zip\n",
      "Opening: nvdcve-1.1-2006.json.zip\n",
      "Opening: nvdcve-1.1-2007.json.zip\n",
      "Opening: nvdcve-1.1-2008.json.zip\n",
      "Opening: nvdcve-1.1-2009.json.zip\n",
      "Opening: nvdcve-1.1-2010.json.zip\n",
      "Opening: nvdcve-1.1-2011.json.zip\n",
      "Opening: nvdcve-1.1-2012.json.zip\n",
      "Opening: nvdcve-1.1-2013.json.zip\n",
      "Opening: nvdcve-1.1-2014.json.zip\n",
      "Opening: nvdcve-1.1-2015.json.zip\n",
      "Opening: nvdcve-1.1-2016.json.zip\n",
      "Opening: nvdcve-1.1-2017.json.zip\n",
      "Opening: nvdcve-1.1-2018.json.zip\n",
      "Opening: nvdcve-1.1-2019.json.zip\n",
      "Opening: nvdcve-1.1-2020.json.zip\n",
      "Opening: nvdcve-1.1-2021.json.zip\n"
     ]
    }
   ],
   "source": [
    "#Extract the JSON files from .zip files.\n",
    "def unzip_data():\n",
    "    files = [f for f in listdir(\"zipFile/\") if isfile(join(\"zipFile/\", f))]\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        print(\"Opening: \" + file)\n",
    "        archive = zipfile.ZipFile(join(\"zipFile/\", file), 'r')\n",
    "        filePath = \"jsonFile\"\n",
    "        if not os.path.exists(filePath):\n",
    "            os.makedirs(filePath)\n",
    "        with archive as f:\n",
    "            f.extractall('jsonFile')\n",
    "unzip_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nvd_dict(year):\n",
    "    filename = join(\"jsonFile/nvdcve-1.1-\" + str(year) + \".json\")\n",
    "    #print(\"Opening: \" + filename)\n",
    "    with open(filename, encoding=\"utf8\") as json_file:\n",
    "    \tcve_dict = json.load(json_file)\n",
    "    return(cve_dict)\n",
    "\n",
    "def generate_CVSSV2csv_for_training():\n",
    "    list = listdir(\"jsonFile/\")\n",
    "    number_files = len(list) - 1\n",
    "    for year in range(2002,2002 + number_files):\n",
    "        year_in_string = str(year)\n",
    "        cve_dict = create_nvd_dict(year)\n",
    "        fileName = 'NVD_'+ year_in_string + '_CVSSV2_train.csv'\n",
    "        with open('trainCVSSV2/' + fileName, 'w', newline='') as f_output:\n",
    "            csv_output = csv.writer(f_output)\n",
    "            csv_output.writerow(['CVE_ID', 'PublishTime','ModifyTime','Report','CVSSV2','AccessVector','AccessComplexity',\n",
    "                             'Authentication','ConfidentialityImpact','IntegrityImpact','AvailabilityImpact'])\n",
    "\n",
    "            for item in cve_dict['CVE_Items']:\n",
    "                cve_id = item['cve']['CVE_data_meta']['ID']\n",
    "                publish = item['publishedDate']\n",
    "                modify = item['lastModifiedDate']\n",
    "                report = item['cve']['description']['description_data'][0]['value']\n",
    "                if not report.find(\"**REJECT**\"):\n",
    "                    continue\n",
    "                if 'baseMetricV2' not in item['impact']:\n",
    "                    continue\n",
    "                elif 'baseMetricV2' in item['impact']:\n",
    "                    cvssv2_base_score = item['impact']['baseMetricV2']['cvssV2']['baseScore']\n",
    "                    accessVector = item['impact']['baseMetricV2']['cvssV2']['accessVector']\n",
    "                    accessComplexity = item['impact']['baseMetricV2']['cvssV2']['accessComplexity']\n",
    "                    authentication = item['impact']['baseMetricV2']['cvssV2']['authentication']\n",
    "                    confidentialityImpact = item['impact']['baseMetricV2']['cvssV2']['confidentialityImpact']\n",
    "                    integrityImpact = item['impact']['baseMetricV2']['cvssV2']['integrityImpact']\n",
    "                    availabilityImpact = item['impact']['baseMetricV2']['cvssV2']['availabilityImpact']\n",
    "\n",
    "                    csv_output.writerow([cve_id, publish, modify,report, cvssv2_base_score,\n",
    "                                 accessVector,accessComplexity, authentication,confidentialityImpact, integrityImpact, availabilityImpact])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_CVSSV2csv_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate training dataset using NVD reports from 2002 to 2020.\n",
    "def generate_CombinedFile():\n",
    "    list = listdir(\"trainCVSSV2/\")\n",
    "    number_files = len(list)-1\n",
    "    dict = []\n",
    "    dict_of_reports = {}\n",
    "    for year in range(2002,2002 + number_files):\n",
    "        year_in_string = str(year)\n",
    "        file_name = 'NVD_'+ year_in_string + '_CVSSV2_train.csv'\n",
    "        dict_of_reports[year_in_string] = []\n",
    "        dict_of_reports[year_in_string] = pd.read_csv(\"trainCVSSV2/\" + file_name)\n",
    "        dict.append(dict_of_reports[year_in_string])\n",
    "    df = pd.concat(dict, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_CombinedFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " CVE_ID                    object\n",
      "PublishTime               object\n",
      "ModifyTime                object\n",
      "Report                    object\n",
      "CVSSV2                   float64\n",
      "AccessVector              object\n",
      "AccessComplexity          object\n",
      "Authentication            object\n",
      "ConfidentialityImpact     object\n",
      "IntegrityImpact           object\n",
      "AvailabilityImpact        object\n",
      "dtype: object\n",
      "Number of vulnerability reports,columns= (132771, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"Schema:\\n\\n\",df.dtypes)\n",
    "print(\"Number of vulnerability reports,columns=\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " CVE_ID                    object\n",
      "PublishTime               object\n",
      "ModifyTime                object\n",
      "Report                    object\n",
      "CVSSV2                   float64\n",
      "AccessVector              object\n",
      "AccessComplexity          object\n",
      "Authentication            object\n",
      "ConfidentialityImpact     object\n",
      "IntegrityImpact           object\n",
      "AvailabilityImpact        object\n",
      "dtype: object\n",
      "Number of vulnerability reports,columns= (114507, 11)\n"
     ]
    }
   ],
   "source": [
    "df = df[~df['Report'].str.contains('REJECT')]\n",
    "# print schema\n",
    "print(\"Schema:\\n\\n\",df.dtypes)\n",
    "print(\"Number of vulnerability reports,columns=\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " CVE_ID                    object\n",
      "PublishTime               object\n",
      "ModifyTime                object\n",
      "Report                    object\n",
      "CVSSV2                   float64\n",
      "AccessVector              object\n",
      "AccessComplexity          object\n",
      "Authentication            object\n",
      "ConfidentialityImpact     object\n",
      "IntegrityImpact           object\n",
      "AvailabilityImpact        object\n",
      "dtype: object\n",
      "Number of vulnerability reports,columns= (150526, 11)\n"
     ]
    }
   ],
   "source": [
    "df = df[~df['Report'].str.contains('REJECT')]\n",
    "# print schema\n",
    "print(\"Schema:\\n\\n\",df.dtypes)\n",
    "print(\"Number of vulnerability reports,columns=\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CVE_ID</th>\n",
       "      <th>PublishTime</th>\n",
       "      <th>ModifyTime</th>\n",
       "      <th>Report</th>\n",
       "      <th>CVSSV2</th>\n",
       "      <th>AccessVector</th>\n",
       "      <th>AccessComplexity</th>\n",
       "      <th>Authentication</th>\n",
       "      <th>ConfidentialityImpact</th>\n",
       "      <th>IntegrityImpact</th>\n",
       "      <th>AvailabilityImpact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79417</th>\n",
       "      <td>CVE-2016-10231</td>\n",
       "      <td>2018-04-04T18:29Z</td>\n",
       "      <td>2018-05-04T19:14Z</td>\n",
       "      <td>An elevation of privilege vulnerability in the...</td>\n",
       "      <td>9.3</td>\n",
       "      <td>NETWORK</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>NONE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146383</th>\n",
       "      <td>CVE-2020-4422</td>\n",
       "      <td>2020-05-14T16:15Z</td>\n",
       "      <td>2021-07-21T11:39Z</td>\n",
       "      <td>IBM i2 Intelligent Analyis Platform 9.2.1 coul...</td>\n",
       "      <td>9.3</td>\n",
       "      <td>NETWORK</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>NONE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                CVE_ID        PublishTime         ModifyTime  \\\n",
       "79417   CVE-2016-10231  2018-04-04T18:29Z  2018-05-04T19:14Z   \n",
       "146383   CVE-2020-4422  2020-05-14T16:15Z  2021-07-21T11:39Z   \n",
       "\n",
       "                                                   Report  CVSSV2  \\\n",
       "79417   An elevation of privilege vulnerability in the...     9.3   \n",
       "146383  IBM i2 Intelligent Analyis Platform 9.2.1 coul...     9.3   \n",
       "\n",
       "       AccessVector AccessComplexity Authentication ConfidentialityImpact  \\\n",
       "79417       NETWORK           MEDIUM           NONE              COMPLETE   \n",
       "146383      NETWORK           MEDIUM           NONE              COMPLETE   \n",
       "\n",
       "       IntegrityImpact AvailabilityImpact  \n",
       "79417         COMPLETE           COMPLETE  \n",
       "146383        COMPLETE           COMPLETE  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):    \n",
    "    # lowercase\n",
    "    text=text.lower()    \n",
    "    #remove tags\n",
    "    text=re.sub(\"<!--?.*?-->\",\"\",text)    \n",
    "    # remove special characters and digits \n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text= re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def _reciprocal_rank(true_labels: list, machine_preds: list):\n",
    "    \"\"\"Compute the reciprocal rank at cutoff k\"\"\"\n",
    "    # add index to list only if machine predicted label exists in true labels\n",
    "    tp_pos_list = [(idx + 1) for idx, r in enumerate(machine_preds) if r in true_labels]\n",
    "    rr = 0\n",
    "    if len(tp_pos_list) > 0:\n",
    "        # for RR we need position of first correct item\n",
    "        first_pos_list = tp_pos_list[0]\n",
    "        # rr = 1/rank\n",
    "        rr = 1 / float(first_pos_list)\n",
    "    return rr\n",
    "\n",
    "def compute_mrr_at_k(items:list):\n",
    "    \"\"\"Compute the MRR (average RR) at cutoff k\"\"\"\n",
    "    rr_total = 0\n",
    "    for item in items:   \n",
    "        rr_at_k = _reciprocal_rank(item[0],item[1])\n",
    "        rr_total = rr_total + rr_at_k\n",
    "        mrr = rr_total / 1/float(len(items))\n",
    "    return mrr\n",
    "\n",
    "def collect_preds(Y_test,Y_preds):\n",
    "    \"\"\"Collect all predictions and ground truth\"\"\"\n",
    "    pred_gold_list=[[[Y_test[idx]],pred] for idx,pred in enumerate(Y_preds)]\n",
    "    return pred_gold_list\n",
    "             \n",
    "def compute_accuracy(eval_items:list):\n",
    "    correct=0\n",
    "    total=0    \n",
    "    for item in eval_items:\n",
    "        true_pred=item[0]\n",
    "        machine_pred=set(item[1])\n",
    "        for cat in true_pred:\n",
    "            if cat in machine_pred:\n",
    "                correct+=1\n",
    "                break\n",
    "    accuracy=correct/float(len(eval_items))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    sns.set_context('talk')\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"src/stopwords.txt\")\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def extract_features(df,field,training_data,testing_data,type=\"binary\"):\n",
    "    \"\"\"Extract features using different methods\"\"\"\n",
    "    logging.info(\"Extracting features and creating vocabulary...\")\n",
    "    \n",
    "    if \"binary\" in type:\n",
    "        # BINARY FEATURE REPRESENTATION\n",
    "        cv= CountVectorizer(binary=True, max_df=0.95)\n",
    "        cv.fit_transform(training_data[field].apply(lambda x:pre_process(x)).tolist())\n",
    "        train_feature_set=cv.transform(training_data[field].apply(lambda x:pre_process(x)).tolist())\n",
    "        test_feature_set=cv.transform(testing_data[field].apply(lambda x:pre_process(x)).tolist())\n",
    "        return train_feature_set,test_feature_set,cv\n",
    "  \n",
    "    elif \"counts\" in type:\n",
    "        # COUNT BASED FEATURE REPRESENTATION\n",
    "        cv= CountVectorizer(binary=False, max_df=0.95)\n",
    "        cv.fit_transform(training_data[field].apply(lambda x:pre_process(x)).tolist())\n",
    "        train_feature_set=cv.transform(training_data[field].apply(lambda x:pre_process(x)).tolist())\n",
    "        test_feature_set=cv.transform(testing_data[field].apply(lambda x:pre_process(x)).tolist())\n",
    "        return train_feature_set,test_feature_set,cv\n",
    "    \n",
    "    else:    \n",
    "        # TF-IDF BASED FEATURE REPRESENTATION\n",
    "        tfidf_vectorizer=TfidfVectorizer(smooth_idf=True,use_idf=True, max_df=0.95)\n",
    "        tfidf_vectorizer.fit_transform(training_data[field].apply(lambda x:pre_process(x)).tolist())\n",
    "        train_feature_set=tfidf_vectorizer.transform(training_data[field].apply(lambda x:pre_process(x)).tolist())\n",
    "        test_feature_set=tfidf_vectorizer.transform(testing_data[field].apply(lambda x:pre_process(x)).tolist())\n",
    "        return train_feature_set,test_feature_set,tfidf_vectorizer\n",
    "    \n",
    "def get_top_k_predictions(model,X_test,k):\n",
    "    \n",
    "    # get probabilities instead of predicted labels, since we want to collect top 3\n",
    "    probs = model.predict_proba(X_test)\n",
    "\n",
    "    # GET TOP K PREDICTIONS BY PROB - note these are just index\n",
    "    best_n = np.argsort(probs, axis=1)[:,-k:]\n",
    "    \n",
    "    # GET CATEGORY OF PREDICTIONS\n",
    "    preds=[[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n",
    "    preds=[ item[::-1] for item in preds]\n",
    "    return preds\n",
    "\n",
    "def train_model(df,field,feature_rep,top_k,label):\n",
    "    \n",
    "    logging.info(\"Starting model training...\")\n",
    "    \n",
    "    # GET A TRAIN TEST SPLIT (set seed for consistent results)\n",
    "    training_data, testing_data = train_test_split(df,random_state = 2000,test_size=0.25)\n",
    "\n",
    "    # GET LABELS\n",
    "    if 'AccessVector' in label:\n",
    "        Y_train=training_data['AccessVector'].values\n",
    "        Y_test=testing_data['AccessVector'].values\n",
    "        classes = ['NETWORK','ADJACENT_NETWORK','LOCAL']\n",
    "    elif 'AccessComplexity' in label:\n",
    "        Y_train=training_data['AccessComplexity'].values\n",
    "        Y_test=testing_data['AccessComplexity'].values\n",
    "        classes = ['HIGH','MEDIUM','LOW']\n",
    "    elif 'Authentication' in label:\n",
    "        Y_train=training_data['Authentication'].values\n",
    "        Y_test=testing_data['Authentication'].values\n",
    "        classes = ['MULTIPLE','SINGLE','NONE']\n",
    "    elif 'ConfidentialityImpact' in label:\n",
    "        Y_train=training_data['ConfidentialityImpact'].values\n",
    "        Y_test=testing_data['ConfidentialityImpact'].values\n",
    "        classes = ['COMPLETE','PARTIAL','NONE']\n",
    "    elif 'IntegrityImpact' in label:\n",
    "        Y_train=training_data['IntegrityImpact'].values\n",
    "        Y_test=testing_data['IntegrityImpact'].values\n",
    "        classes = ['COMPLETE','PARTIAL','NONE']\n",
    "    elif 'AvailabilityImpact' in label:\n",
    "        Y_train=training_data['AvailabilityImpact'].values\n",
    "        Y_test=testing_data['AvailabilityImpact'].values\n",
    "        classes = ['COMPLETE','PARTIAL','NONE']\n",
    "    # GET FEATURES\n",
    "    X_train,X_test,feature_transformer=extract_features(df,field,training_data,testing_data,type=feature_rep)\n",
    "\n",
    "    # INIT LOGISTIC REGRESSION CLASSIFIER\n",
    "    logging.info(\"Training a Logistic Regression Model...\")\n",
    "    scikit_log_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "    model=scikit_log_reg.fit(X_train,Y_train)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(14, 12, forward=True)\n",
    "    \n",
    "    plot_confusion_matrix(Y_test,preds, classes,normalize=True,\n",
    "                      title='Normalized Confusion matrix of '+label)\n",
    "    plt.grid(False)\n",
    "    precision, recall, fscore, support = score(Y_test,preds)\n",
    "    \n",
    "    # GET TOP K PREDICTIONS\n",
    "    preds=get_top_k_predictions(model,X_test,top_k)\n",
    "    \n",
    "    # GET PREDICTED VALUES AND GROUND TRUTH INTO A LIST OF LISTS - for ease of evaluation\n",
    "    eval_items=collect_preds(Y_test,preds)\n",
    "    \n",
    "    # GET EVALUATION NUMBERS ON TEST SET -- HOW DID WE DO?\n",
    "    logging.info(\"Starting evaluation...\")\n",
    "    accuracy=compute_accuracy(eval_items)\n",
    "    mrr_at_k=compute_mrr_at_k(eval_items)\n",
    "    logging.info(\"Done training and evaluation.\")\n",
    "    \n",
    "    return model,feature_transformer,accuracy,mrr_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 18:56:43,062 : INFO : Starting model training...\n",
      "2021-11-19 18:56:43,220 : INFO : Extracting features and creating vocabulary...\n",
      "2021-11-19 18:57:51,119 : INFO : Training a Logistic Regression Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 18:58:15,912 : INFO : Starting evaluation...\n",
      "2021-11-19 18:58:15,996 : INFO : Done training and evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy=0.9500159438775511; MRR=0.9500159438775511\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "field='Report'\n",
    "feature_rep='tfidf'\n",
    "top_k=1\n",
    "label='AccessVector'\n",
    "\n",
    "modelAccessVectorTFIDF,transformerAccessVectorTFIDF,accuracy,mrr_at_k=train_model(df,field=field,feature_rep=feature_rep,top_k=top_k,label=label)\n",
    "print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy,mrr_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 18:58:16,049 : INFO : Starting model training...\n",
      "2021-11-19 18:58:16,118 : INFO : Extracting features and creating vocabulary...\n",
      "2021-11-19 18:58:41,331 : INFO : Training a Logistic Regression Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 18:59:01,696 : INFO : Starting evaluation...\n",
      "2021-11-19 18:59:01,788 : INFO : Done training and evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy=0.8405346513605442; MRR=0.8405346513605442\n"
     ]
    }
   ],
   "source": [
    "field='Report'\n",
    "feature_rep='tfidf'\n",
    "top_k=1\n",
    "label='AccessComplexity'\n",
    "\n",
    "modelAccessComplexityTFIDF,transformerAccessComplexityTFIDF,accuracy,mrr_at_k=train_model(df,field=field,feature_rep=feature_rep,top_k=top_k,label=label)\n",
    "print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy,mrr_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 18:59:01,830 : INFO : Starting model training...\n",
      "2021-11-19 18:59:01,910 : INFO : Extracting features and creating vocabulary...\n",
      "2021-11-19 18:59:27,875 : INFO : Training a Logistic Regression Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 18:59:39,837 : INFO : Starting evaluation...\n",
      "2021-11-19 18:59:39,915 : INFO : Done training and evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy=0.9405027636054422; MRR=0.9405027636054422\n"
     ]
    }
   ],
   "source": [
    "field='Report'\n",
    "feature_rep='tfidf'\n",
    "top_k=1\n",
    "label='Authentication'\n",
    "\n",
    "modelAuthenticationTFIDF,transformerAuthenticationTFIDF,accuracy,mrr_at_k=train_model(df,field=field,feature_rep=feature_rep,top_k=top_k,label=label)\n",
    "print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy,mrr_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 18:59:39,964 : INFO : Starting model training...\n",
      "2021-11-19 18:59:40,055 : INFO : Extracting features and creating vocabulary...\n",
      "2021-11-19 19:00:11,725 : INFO : Training a Logistic Regression Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 19:00:48,163 : INFO : Starting evaluation...\n",
      "2021-11-19 19:00:48,238 : INFO : Done training and evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy=0.8241390306122449; MRR=0.8241390306122449\n"
     ]
    }
   ],
   "source": [
    "field='Report'\n",
    "feature_rep='tfidf'\n",
    "top_k=1\n",
    "label='ConfidentialityImpact'\n",
    "\n",
    "modelConfidentialityTFIDF,transformerConfidentialityTFIDF,accuracy,mrr_at_k=train_model(df,field=field,feature_rep=feature_rep,top_k=top_k,label=label)\n",
    "print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy,mrr_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 19:00:48,307 : INFO : Starting model training...\n",
      "2021-11-19 19:00:48,389 : INFO : Extracting features and creating vocabulary...\n",
      "2021-11-19 19:01:15,179 : INFO : Training a Logistic Regression Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 19:01:33,624 : INFO : Starting evaluation...\n",
      "2021-11-19 19:01:33,693 : INFO : Done training and evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy=0.841969600340136; MRR=0.841969600340136\n"
     ]
    }
   ],
   "source": [
    "field='Report'\n",
    "feature_rep='tfidf'\n",
    "top_k=1\n",
    "label='IntegrityImpact'\n",
    "\n",
    "modelIntegrityTFIDF,transformerIntegrityTFIDF,accuracy,mrr_at_k=train_model(df,field=field,feature_rep=feature_rep,top_k=top_k,label=label)\n",
    "print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy,mrr_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 19:01:33,747 : INFO : Starting model training...\n",
      "2021-11-19 19:01:33,835 : INFO : Extracting features and creating vocabulary...\n",
      "2021-11-19 19:01:58,456 : INFO : Training a Logistic Regression Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 19:02:17,592 : INFO : Starting evaluation...\n",
      "2021-11-19 19:02:17,673 : INFO : Done training and evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy=0.809125212585034; MRR=0.809125212585034\n"
     ]
    }
   ],
   "source": [
    "field='Report'\n",
    "feature_rep='tfidf'\n",
    "top_k=1\n",
    "label='AvailabilityImpact'\n",
    "\n",
    "modelAvailabilityTFIDF,transformerAvailabilityTFIDF,accuracy,mrr_at_k=train_model(df,field=field,feature_rep=feature_rep,top_k=top_k,label=label)\n",
    "print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy,mrr_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_impactScore(document):\n",
    "    test_features_Confidentiality=transformerConfidentialityTFIDF.transform(document)\n",
    "    confidentiality_label=get_top_k_predictions(modelConfidentialityTFIDF,test_features_Confidentiality,1)\n",
    "    confidentiality_label=eval(str(confidentiality_label).strip('[]'))\n",
    "    if confidentiality_label=='COMPLETE':\n",
    "        confidentiality_value=0.66\n",
    "    elif confidentiality_label=='PARTIAL': \n",
    "        confidentiality_value=0.275\n",
    "    else:\n",
    "        confidentiality_value=0\n",
    "\n",
    "    test_features_Integrity=transformerIntegrityTFIDF.transform(document)\n",
    "    integrity_label=get_top_k_predictions(modelIntegrityTFIDF,test_features_Integrity,1)\n",
    "    integrity_label=eval(str(integrity_label).strip('[]'))\n",
    "    if integrity_label=='COMPLETE':\n",
    "        integrity_value=0.66\n",
    "    elif integrity_label=='PARTIAL': \n",
    "        integrity_value=0.275\n",
    "    else:\n",
    "        integrity_value=0\n",
    "        \n",
    "    test_features_Availability=transformerAvailabilityTFIDF.transform(document)\n",
    "    availability_label=get_top_k_predictions(modelAvailabilityTFIDF,test_features_Availability,1)\n",
    "    availability_label=eval(str(availability_label).strip('[]'))\n",
    "    if availability_label=='COMPLETE':\n",
    "        availability_value=0.66\n",
    "    elif availability_label=='PARTIAL': \n",
    "        availability_value=0.275\n",
    "    else:\n",
    "        availability_value=0\n",
    "    \n",
    "    impactScore=((1-(1-confidentiality_value)*(1-integrity_value)*(1-availability_value)))*10.41\n",
    "    return impactScore\n",
    "\n",
    "def calculate_impactIndex(document):\n",
    "    Impact=calculate_impactScore(document)\n",
    "    if Impact == 0:\n",
    "        impactIndex=0\n",
    "    else:\n",
    "        impactIndex=1.176\n",
    "    return impactIndex\n",
    "\n",
    "def calculate_exploitabilityScore(document):\n",
    "    test_features_AccessVector=transformerAccessVectorTFIDF.transform(document)\n",
    "    accessVector_label=get_top_k_predictions(modelAccessVectorTFIDF,test_features_AccessVector,1)\n",
    "    accessVector_label=eval(str(accessVector_label).strip('[]'))\n",
    "    if accessVector_label=='NETWORK':\n",
    "        accessVector_value=1\n",
    "    elif accessVector_label=='ADJACENT_NETWORK':\n",
    "        accessVector_value=0.646\n",
    "    else:\n",
    "        accessVector_value=0.395\n",
    "        \n",
    "    test_features_AccessComplexity=transformerAccessComplexityTFIDF.transform(document)\n",
    "    accessComplexity_label=get_top_k_predictions(modelAccessComplexityTFIDF,test_features_AccessComplexity,1)\n",
    "    accessComplexity_label=eval(str(accessComplexity_label).strip('[]'))\n",
    "    if accessComplexity_label=='LOW':\n",
    "        accessComplexity_value=0.71\n",
    "    elif accessComplexity_label=='HIGH':\n",
    "        accessComplexity_value=0.35\n",
    "    else:\n",
    "        accessComplexity_value=0.61\n",
    "        \n",
    "    test_features_Authentication=transformerAuthenticationTFIDF.transform(document)\n",
    "    authentication_label=get_top_k_predictions(modelAuthenticationTFIDF,test_features_Authentication,1)\n",
    "    authentication_label=eval(str(authentication_label).strip('[]'))\n",
    "    if authentication_label=='MULTIPLE':\n",
    "        authentication_value=0.45\n",
    "    elif authentication_label=='SINGLE':\n",
    "        authentication_value=0.56\n",
    "    else:\n",
    "        authentication_value=0.704\n",
    "    \n",
    "    exploitabilityScore=20*accessVector_value*accessComplexity_value*authentication_value\n",
    "    return exploitabilityScore\n",
    "\n",
    "import math\n",
    "def round_up(n, decimals=1):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.ceil(n * multiplier) / multiplier\n",
    "def calculate_v2baseScore(document):\n",
    "    exploitabilityScore=calculate_exploitabilityScore(document)\n",
    "    impactScore=calculate_impactScore(document)\n",
    "    impactIndex=calculate_impactIndex(document)\n",
    "    baseScore=round_up((0.6*(impactScore)+0.4*(exploitabilityScore)-1.5)*impactIndex)\n",
    "    return baseScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13732, 11)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NVD reports in 2021 are used as validation.\n",
    "df2021 = pd.read_csv('validateCVSSV2/NVD_2021_CVSSV2_train.csv')\n",
    "df2021.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def train_cvssv2_model(df):\n",
    "\n",
    "    docs=df2021['Report'].tolist()\n",
    "    preds=[]\n",
    "    for i in range(0,len(df2021)):\n",
    "        pred_score=calculate_v2baseScore([docs[i]])\n",
    "        preds.append(str(pred_score))\n",
    "    \n",
    "    # GET EVALUATION NUMBERS ON TEST SET -- HOW DID WE DO?\n",
    "    logging.info(\"Starting evaluation...\")\n",
    "    \n",
    "    logging.info(\"Done training and evaluation.\")\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 19:48:14,369 : INFO : Starting evaluation...\n",
      "2021-11-19 19:48:14,371 : INFO : Done training and evaluation.\n"
     ]
    }
   ],
   "source": [
    "preds = train_cvssv2_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels=[]\n",
    "y_test = df2021['CVSSV2'].values\n",
    "for item in y_test:\n",
    "    item = str(item)\n",
    "    y_test_labels.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeverityLabel(score):\n",
    "    if score >= 0 and score <=3.9:\n",
    "        severityLabel = 'Low'\n",
    "    elif score >= 4.0 and score <=6.9:\n",
    "        severityLabel = 'Medium'\n",
    "    elif score >= 7.0 and score <=10.0:\n",
    "        severityLabel = 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_SeverityLabelList = []\n",
    "for item in preds:\n",
    "    score = float(item)\n",
    "    severityLabel = getSeverityLabel(score)\n",
    "    preds_SeverityLabelList.append(severityLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13732"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_SeverityLabelList = []\n",
    "for item in y_test_labels:\n",
    "    test_score = float(item)\n",
    "    test_severityLabel = getSeverityLabel(test_score)\n",
    "    test_SeverityLabelList.append(test_severityLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct=0\n",
    "total=0\n",
    "    \n",
    "for i in range(0,len(df2021)):\n",
    "    true_pred=test_SeverityLabelList[i]\n",
    "    machine_pred=preds_SeverityLabelList[i]\n",
    "    if true_pred == machine_pred:\n",
    "        correct+=1\n",
    "    \n",
    "accuracy=correct/float(len(preds))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8569035828721235"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct=0\n",
    "total=0   \n",
    "for i in range(0,len(df2021)):\n",
    "    true_pred=y_test_labels[i]\n",
    "    machine_pred=preds[i]\n",
    "    true_pred=float(true_pred)\n",
    "    machine_pred=float(machine_pred)\n",
    "    try:\n",
    "        if true_pred == 0 and machine_pred != 0:\n",
    "            distance = machine_pred\n",
    "        elif true_pred != 0 and machine_pred == 0:\n",
    "            distance = true_pred\n",
    "        elif true_pred == machine_pred:\n",
    "            distance = 0\n",
    "        elif true_pred < machine_pred:\n",
    "            distance = (machine_pred - true_pred)/true_pred\n",
    "        else:\n",
    "            distance = (true_pred - machine_pred)/true_pred\n",
    "    except ZeroDivisionError:\n",
    "        print(\"true_pred = 0\")\n",
    "    if float(distance) <= 0.5:\n",
    "        correct+=1\n",
    "accuracy=correct/float(len(preds))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVSS V2 base-score: 10.0\n",
      "CVSS V2 impact-score: 10.00084536\n",
      "CVSS V2 explotability-score: 9.996799999999999\n"
     ]
    }
   ],
   "source": [
    "#Prediction for CVE-2019-14931\n",
    "doc1=[\"An issue was discovered on Mitsubishi Electric ME-RTU devices through 2.02 and INEA ME-RTU devices through 3.0. An unauthenticated remote OS Command Injection vulnerability allows an attacker to execute arbitrary commands on the RTU due to the passing of unsafe user supplied data to the RTU's system shell. Functionality in mobile.php provides users with the ability to ping sites or IP addresses via Mobile Connection Test. When the Mobile Connection Test is submitted, action.php is called to execute the test. An attacker can use a shell command separator (;) in the host variable to execute operating system commands upon submitting the test data.\"]\n",
    "doc1_impactScore=calculate_impactScore(doc1)\n",
    "doc1_impactIndex=calculate_impactIndex(doc1)\n",
    "doc1_explotabilityScore=calculate_exploitabilityScore(doc1)\n",
    "doc1_cvssV2Score=calculate_v2baseScore(doc1)\n",
    "accessVector_label,accessComplexity_label,authentication_label,confidentiality_label,integrity_label,availability_label,\n",
    "print(\"CVSS V2 base-score: \" + str(doc1_cvssV2Score))\n",
    "print(\"CVSS V2 impact-score: \" + str(doc1_impactScore))\n",
    "print(\"CVSS V2 explotability-score: \" + str(doc1_explotabilityScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVSS V2 base-score: 5.0\n",
      "CVSS V2 impact-score: 2.86275\n",
      "CVSS V2 explotability-score: 9.996799999999999\n"
     ]
    }
   ],
   "source": [
    "#Prediction for CVE-2013-6143\n",
    "doc2=[\"The Schneider Electric Telvent SAGE 3030 RTU with firmware C3413-500-001D3_P4 and C3413-500-001F0_PB allows remote attackers to cause a denial of service (temporary outage and CPU consumption) via malformed DNP3 traffic.\"] \n",
    "doc2_impactScore=calculate_impactScore(doc2)\n",
    "doc2_impactIndex=calculate_impactIndex(doc2)\n",
    "doc2_explotabilityScore=calculate_exploitabilityScore(doc2)\n",
    "doc2_cvssV2Score=calculate_v2baseScore(doc2)\n",
    "accessVector_label,accessComplexity_label,authentication_label,confidentiality_label,integrity_label,availability_label,\n",
    "print(\"CVSS V2 base-score: \" + str(doc2_cvssV2Score))\n",
    "print(\"CVSS V2 impact-score: \" + str(doc2_impactScore))\n",
    "print(\"CVSS V2 explotability-score: \" + str(doc2_explotabilityScore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
